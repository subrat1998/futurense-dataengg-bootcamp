subrat@subrat-virtual-machine:~$ spark-submit  --packages org.apache.spark:spark-avro_2.12:3.3.2 BankmarketApplication.py
23/03/08 11:15:40 WARN Utils: Your hostname, subrat-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.253.128 instead (on interface ens33)
23/03/08 11:15:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/subrat/.ivy2/cache
The jars for the packages stored in: /home/subrat/.ivy2/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-9cf30fdd-004a-4440-95b7-1a1d8f7469cc;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (958ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (507ms)
:: resolution report :: resolve 3492ms :: artifacts dl 1473ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   2   |   2   |   0   ||   3   |   2   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-9cf30fdd-004a-4440-95b7-1a1d8f7469cc
	confs: [default]
	2 artifacts copied, 1 already retrieved (303kB/20ms)
23/03/08 11:15:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/03/08 11:15:47 INFO SparkContext: Running Spark version 3.3.2
23/03/08 11:15:47 INFO ResourceUtils: ==============================================================
23/03/08 11:15:47 INFO ResourceUtils: No custom resources configured for spark.driver.
23/03/08 11:15:47 INFO ResourceUtils: ==============================================================
23/03/08 11:15:47 INFO SparkContext: Submitted application: BankMarketApplication
23/03/08 11:15:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/03/08 11:15:47 INFO ResourceProfile: Limiting resource is cpu
23/03/08 11:15:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/03/08 11:15:48 INFO SecurityManager: Changing view acls to: subrat
23/03/08 11:15:48 INFO SecurityManager: Changing modify acls to: subrat
23/03/08 11:15:48 INFO SecurityManager: Changing view acls groups to: 
23/03/08 11:15:48 INFO SecurityManager: Changing modify acls groups to: 
23/03/08 11:15:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(subrat); groups with view permissions: Set(); users  with modify permissions: Set(subrat); groups with modify permissions: Set()
23/03/08 11:15:48 INFO Utils: Successfully started service 'sparkDriver' on port 46427.
23/03/08 11:15:48 INFO SparkEnv: Registering MapOutputTracker
23/03/08 11:15:48 INFO SparkEnv: Registering BlockManagerMaster
23/03/08 11:15:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/03/08 11:15:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/03/08 11:15:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/03/08 11:15:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59de4137-f7fc-4bb3-aaec-45c3ea4a94df
23/03/08 11:15:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
23/03/08 11:15:48 INFO SparkEnv: Registering OutputCommitCoordinator
23/03/08 11:15:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/03/08 11:15:49 INFO SparkContext: Added JAR file:///home/subrat/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://192.168.253.128:46427/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO SparkContext: Added JAR file:///home/subrat/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://192.168.253.128:46427/jars/org.tukaani_xz-1.9.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO SparkContext: Added JAR file:///home/subrat/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://192.168.253.128:46427/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO SparkContext: Added file file:///home/subrat/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at file:///home/subrat/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO Utils: Copying /home/subrat/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.apache.spark_spark-avro_2.12-3.3.2.jar
23/03/08 11:15:49 INFO SparkContext: Added file file:///home/subrat/.ivy2/jars/org.tukaani_xz-1.9.jar at file:///home/subrat/.ivy2/jars/org.tukaani_xz-1.9.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO Utils: Copying /home/subrat/.ivy2/jars/org.tukaani_xz-1.9.jar to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.tukaani_xz-1.9.jar
23/03/08 11:15:49 INFO SparkContext: Added file file:///home/subrat/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///home/subrat/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO Utils: Copying /home/subrat/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.spark-project.spark_unused-1.0.0.jar
23/03/08 11:15:49 INFO Executor: Starting executor ID driver on host 192.168.253.128
23/03/08 11:15:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/03/08 11:15:49 INFO Executor: Fetching file:///home/subrat/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO Utils: /home/subrat/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.spark-project.spark_unused-1.0.0.jar
23/03/08 11:15:49 INFO Executor: Fetching file:///home/subrat/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO Utils: /home/subrat/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar has been previously copied to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.apache.spark_spark-avro_2.12-3.3.2.jar
23/03/08 11:15:49 INFO Executor: Fetching file:///home/subrat/.ivy2/jars/org.tukaani_xz-1.9.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO Utils: /home/subrat/.ivy2/jars/org.tukaani_xz-1.9.jar has been previously copied to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.tukaani_xz-1.9.jar
23/03/08 11:15:49 INFO Executor: Fetching spark://192.168.253.128:46427/jars/org.tukaani_xz-1.9.jar with timestamp 1678254347780
23/03/08 11:15:49 INFO TransportClientFactory: Successfully created connection to /192.168.253.128:46427 after 94 ms (0 ms spent in bootstraps)
23/03/08 11:15:49 INFO Utils: Fetching spark://192.168.253.128:46427/jars/org.tukaani_xz-1.9.jar to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/fetchFileTemp6728958049725986341.tmp
23/03/08 11:15:50 INFO Utils: /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/fetchFileTemp6728958049725986341.tmp has been previously copied to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.tukaani_xz-1.9.jar
23/03/08 11:15:50 INFO Executor: Adding file:/tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.tukaani_xz-1.9.jar to class loader
23/03/08 11:15:50 INFO Executor: Fetching spark://192.168.253.128:46427/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1678254347780
23/03/08 11:15:50 INFO Utils: Fetching spark://192.168.253.128:46427/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/fetchFileTemp9071612352061235235.tmp
23/03/08 11:15:50 INFO Utils: /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/fetchFileTemp9071612352061235235.tmp has been previously copied to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.apache.spark_spark-avro_2.12-3.3.2.jar
23/03/08 11:15:50 INFO Executor: Adding file:/tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/03/08 11:15:50 INFO Executor: Fetching spark://192.168.253.128:46427/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1678254347780
23/03/08 11:15:50 INFO Utils: Fetching spark://192.168.253.128:46427/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/fetchFileTemp1387085807405612850.tmp
23/03/08 11:15:50 INFO Utils: /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/fetchFileTemp1387085807405612850.tmp has been previously copied to /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.spark-project.spark_unused-1.0.0.jar
23/03/08 11:15:50 INFO Executor: Adding file:/tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/userFiles-1c52dcd9-b6a1-4e8f-aed6-0544c891ecdd/org.spark-project.spark_unused-1.0.0.jar to class loader
23/03/08 11:15:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36715.
23/03/08 11:15:50 INFO NettyBlockTransferService: Server created on 192.168.253.128:36715
23/03/08 11:15:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/03/08 11:15:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.253.128, 36715, None)
23/03/08 11:15:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.253.128:36715 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.253.128, 36715, None)
23/03/08 11:15:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.253.128, 36715, None)
23/03/08 11:15:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.253.128, 36715, None)
23/03/08 11:15:50 INFO SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/local-1678254349611.inprogress
23/03/08 11:15:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/03/08 11:15:51 INFO SharedState: Warehouse path is 'file:/home/subrat/spark-warehouse'.
23/03/08 11:15:52 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
23/03/08 11:15:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 11:15:56 INFO FileSourceStrategy: Pushed Filters: 
23/03/08 11:15:56 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
23/03/08 11:15:56 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 11:15:57 INFO CodeGenerator: Code generated in 221.222506 ms
23/03/08 11:15:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 353.4 KiB, free 366.0 MiB)
23/03/08 11:15:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.9 MiB)
23/03/08 11:15:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.253.128:36715 (size: 34.8 KiB, free: 366.3 MiB)
23/03/08 11:15:57 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
23/03/08 11:15:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4424930 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:15:57 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
23/03/08 11:15:57 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:15:57 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
23/03/08 11:15:57 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:15:57 INFO DAGScheduler: Missing parents: List()
23/03/08 11:15:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:15:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.9 KiB, free 365.9 MiB)
23/03/08 11:15:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 365.9 MiB)
23/03/08 11:15:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.253.128:36715 (size: 5.9 KiB, free: 366.3 MiB)
23/03/08 11:15:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
23/03/08 11:15:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:15:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/03/08 11:15:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()
23/03/08 11:15:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/03/08 11:15:58 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 0-4424930, partition values: [empty row]
23/03/08 11:15:58 INFO CodeGenerator: Code generated in 49.938845 ms
23/03/08 11:15:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1658 bytes result sent to driver
23/03/08 11:15:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 638 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:15:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/03/08 11:15:58 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.899 s
23/03/08 11:15:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:15:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/03/08 11:15:58 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.021799 s
23/03/08 11:15:58 INFO CodeGenerator: Code generated in 9.123365 ms
23/03/08 11:15:58 INFO FileSourceStrategy: Pushed Filters: 
23/03/08 11:15:58 INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 11:15:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 11:15:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 353.4 KiB, free 365.6 MiB)
23/03/08 11:15:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.5 MiB)
23/03/08 11:15:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.253.128:36715 (size: 34.8 KiB, free: 366.2 MiB)
23/03/08 11:15:58 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
23/03/08 11:15:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4424930 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:15:58 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
23/03/08 11:15:58 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions
23/03/08 11:15:58 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
23/03/08 11:15:58 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:15:58 INFO DAGScheduler: Missing parents: List()
23/03/08 11:15:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:15:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.3 KiB, free 365.5 MiB)
23/03/08 11:15:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 365.5 MiB)
23/03/08 11:15:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.253.128:36715 (size: 12.1 KiB, free: 366.2 MiB)
23/03/08 11:15:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
23/03/08 11:15:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
23/03/08 11:15:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
23/03/08 11:15:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()
23/03/08 11:15:58 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (192.168.253.128, executor driver, partition 1, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()
23/03/08 11:15:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/03/08 11:15:58 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/03/08 11:15:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.253.128:36715 in memory (size: 5.9 KiB, free: 366.2 MiB)
23/03/08 11:15:59 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 0-4424930, partition values: [empty row]
23/03/08 11:15:59 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 4424930-4655556, partition values: [empty row]
23/03/08 11:15:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.253.128:36715 in memory (size: 34.8 KiB, free: 366.3 MiB)
23/03/08 11:15:59 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1685 bytes result sent to driver
23/03/08 11:15:59 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 446 ms on 192.168.253.128 (executor driver) (1/2)
23/03/08 11:15:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1685 bytes result sent to driver
23/03/08 11:15:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 768 ms on 192.168.253.128 (executor driver) (2/2)
23/03/08 11:15:59 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.847 s
23/03/08 11:15:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:15:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/03/08 11:15:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/03/08 11:15:59 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.862135 s
23/03/08 11:16:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(y),EqualTo(y,yes)
23/03/08 11:16:00 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(y#33),(y#33 = yes)
23/03/08 11:16:00 INFO FileSourceStrategy: Output Data Schema: struct<age: int, y: string>
23/03/08 11:16:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.253.128:36715 in memory (size: 12.1 KiB, free: 366.3 MiB)
23/03/08 11:16:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.253.128:36715 in memory (size: 34.8 KiB, free: 366.3 MiB)
23/03/08 11:16:00 INFO CodeGenerator: Code generated in 226.618698 ms
23/03/08 11:16:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 353.3 KiB, free 366.0 MiB)
23/03/08 11:16:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.9 MiB)
23/03/08 11:16:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.253.128:36715 (size: 34.8 KiB, free: 366.3 MiB)
23/03/08 11:16:00 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0
23/03/08 11:16:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4424930 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:16:00 INFO DAGScheduler: Registering RDD 13 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
23/03/08 11:16:00 INFO DAGScheduler: Got map stage job 2 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions
23/03/08 11:16:00 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (showString at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:00 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:16:00 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:00 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 44.2 KiB, free 365.9 MiB)
23/03/08 11:16:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 365.9 MiB)
23/03/08 11:16:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.253.128:36715 (size: 20.2 KiB, free: 366.2 MiB)
23/03/08 11:16:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
23/03/08 11:16:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
23/03/08 11:16:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()
23/03/08 11:16:00 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (192.168.253.128, executor driver, partition 1, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()
23/03/08 11:16:00 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
23/03/08 11:16:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
23/03/08 11:16:01 INFO CodeGenerator: Code generated in 27.869974 ms
23/03/08 11:16:01 INFO CodeGenerator: Code generated in 23.917325 ms
23/03/08 11:16:01 INFO CodeGenerator: Code generated in 36.690035 ms
23/03/08 11:16:01 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 4424930-4655556, partition values: [empty row]
23/03/08 11:16:01 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 0-4424930, partition values: [empty row]
23/03/08 11:16:01 INFO CodeGenerator: Code generated in 39.200334 ms
23/03/08 11:16:01 INFO CodeGenerator: Code generated in 24.298067 ms
23/03/08 11:16:01 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 2829 bytes result sent to driver
23/03/08 11:16:01 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 709 ms on 192.168.253.128 (executor driver) (1/2)
23/03/08 11:16:01 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 2786 bytes result sent to driver
23/03/08 11:16:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 894 ms on 192.168.253.128 (executor driver) (2/2)
23/03/08 11:16:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
23/03/08 11:16:01 INFO DAGScheduler: ShuffleMapStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.937 s
23/03/08 11:16:01 INFO DAGScheduler: looking for newly runnable stages
23/03/08 11:16:01 INFO DAGScheduler: running: Set()
23/03/08 11:16:01 INFO DAGScheduler: waiting: Set()
23/03/08 11:16:01 INFO DAGScheduler: failed: Set()
23/03/08 11:16:01 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 11:16:01 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/03/08 11:16:02 INFO CodeGenerator: Code generated in 37.596431 ms
23/03/08 11:16:02 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
23/03/08 11:16:02 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:16:02 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
23/03/08 11:16:02 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 41.9 KiB, free 365.8 MiB)
23/03/08 11:16:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 365.8 MiB)
23/03/08 11:16:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.253.128:36715 (size: 19.9 KiB, free: 366.2 MiB)
23/03/08 11:16:02 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:16:02 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/03/08 11:16:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (192.168.253.128, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
23/03/08 11:16:02 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
23/03/08 11:16:02 INFO ShuffleBlockFetcherIterator: Getting 2 (656.0 B) non-empty blocks including 2 (656.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/03/08 11:16:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
23/03/08 11:16:02 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 4003 bytes result sent to driver
23/03/08 11:16:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 133 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:16:02 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/03/08 11:16:02 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.166 s
23/03/08 11:16:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:16:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/03/08 11:16:02 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.197168 s
23/03/08 11:16:02 INFO CodeGenerator: Code generated in 20.848452 ms
+-----------+------------------+
|  age_group|subscription_count|
+-----------+------------------+
| Youngsters|              2939|
|    Seniors|               502|
|  Teenagers|                18|
|MiddleAgers|              1830|
+-----------+------------------+

23/03/08 11:16:02 INFO FileSourceStrategy: Pushed Filters: IsNotNull(y),EqualTo(y,yes)
23/03/08 11:16:02 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(y#33),(y#33 = yes)
23/03/08 11:16:02 INFO FileSourceStrategy: Output Data Schema: struct<age: int, y: string>
23/03/08 11:16:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 11:16:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 353.3 KiB, free 365.5 MiB)
23/03/08 11:16:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.4 MiB)
23/03/08 11:16:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.253.128:36715 (size: 34.8 KiB, free: 366.2 MiB)
23/03/08 11:16:02 INFO SparkContext: Created broadcast 7 from parquet at NativeMethodAccessorImpl.java:0
23/03/08 11:16:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4424930 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:16:02 INFO DAGScheduler: Registering RDD 20 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1
23/03/08 11:16:02 INFO DAGScheduler: Got map stage job 4 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions
23/03/08 11:16:02 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:02 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:16:02 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:02 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 44.3 KiB, free 365.4 MiB)
23/03/08 11:16:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 365.4 MiB)
23/03/08 11:16:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.253.128:36715 (size: 20.3 KiB, free: 366.2 MiB)
23/03/08 11:16:02 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:02 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
23/03/08 11:16:02 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
23/03/08 11:16:02 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()
23/03/08 11:16:02 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (192.168.253.128, executor driver, partition 1, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()
23/03/08 11:16:02 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
23/03/08 11:16:02 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
23/03/08 11:16:02 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 4424930-4655556, partition values: [empty row]
23/03/08 11:16:02 INFO FileScanRDD: Reading File path: file:///home/subrat/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv, range: 0-4424930, partition values: [empty row]
23/03/08 11:16:02 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2786 bytes result sent to driver
23/03/08 11:16:02 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 149 ms on 192.168.253.128 (executor driver) (1/2)
23/03/08 11:16:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2786 bytes result sent to driver
23/03/08 11:16:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 278 ms on 192.168.253.128 (executor driver) (2/2)
23/03/08 11:16:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/03/08 11:16:03 INFO DAGScheduler: ShuffleMapStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.313 s
23/03/08 11:16:03 INFO DAGScheduler: looking for newly runnable stages
23/03/08 11:16:03 INFO DAGScheduler: running: Set()
23/03/08 11:16:03 INFO DAGScheduler: waiting: Set()
23/03/08 11:16:03 INFO DAGScheduler: failed: Set()
23/03/08 11:16:03 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 11:16:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 11:16:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 11:16:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 11:16:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 11:16:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 11:16:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 11:16:03 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/03/08 11:16:03 INFO CodeGenerator: Code generated in 51.965313 ms
23/03/08 11:16:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
23/03/08 11:16:03 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:16:03 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/03/08 11:16:03 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 243.5 KiB, free 365.1 MiB)
23/03/08 11:16:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 90.4 KiB, free 365.0 MiB)
23/03/08 11:16:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.253.128:36715 (size: 90.4 KiB, free: 366.1 MiB)
23/03/08 11:16:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:16:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/03/08 11:16:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8) (192.168.253.128, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
23/03/08 11:16:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 8)
23/03/08 11:16:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.253.128:36715 in memory (size: 34.8 KiB, free: 366.1 MiB)
23/03/08 11:16:03 INFO ShuffleBlockFetcherIterator: Getting 2 (656.0 B) non-empty blocks including 2 (656.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/03/08 11:16:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
23/03/08 11:16:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.253.128:36715 in memory (size: 19.9 KiB, free: 366.1 MiB)
23/03/08 11:16:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 11:16:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 11:16:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 11:16:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 11:16:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 11:16:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 11:16:03 INFO CodecConfig: Compression: SNAPPY
23/03/08 11:16:03 INFO CodecConfig: Compression: SNAPPY
23/03/08 11:16:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.253.128:36715 in memory (size: 20.2 KiB, free: 366.2 MiB)
23/03/08 11:16:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.253.128:36715 in memory (size: 20.3 KiB, free: 366.2 MiB)
23/03/08 11:16:04 INFO ParquetOutputFormat: Parquet block size to 134217728
23/03/08 11:16:04 INFO ParquetOutputFormat: Validation is off
23/03/08 11:16:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
23/03/08 11:16:04 INFO ParquetOutputFormat: Parquet properties are:
Parquet page size to 1048576
Parquet dictionary page size to 1048576
Dictionary is true
Writer version is: PARQUET_1_0
Page size checking is: estimated
Min row count for page size check is: 100
Max row count for page size check is: 10000
Truncate length for column indexes is: 64
Truncate length for statistics min/max  is: 2147483647
Bloom filter enabled: false
Max Bloom filter size for a column is 1048576
Bloom filter expected number of distinct values are: null
Page row count limit to 20000
Writing page checksums is: on
23/03/08 11:16:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age_group",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "subscription_count",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required binary age_group (STRING);
  required int64 subscription_count;
}

       
23/03/08 11:16:04 INFO CodecPool: Got brand-new compressor [.snappy]
23/03/08 11:16:05 INFO FileOutputCommitter: Saved output of task 'attempt_202303081116036940450557780958073_0007_m_000000_8' to file:/home/subrat/age_subscription_count.parquet/_temporary/0/task_202303081116036940450557780958073_0007_m_000000
23/03/08 11:16:05 INFO SparkHadoopMapRedUtil: attempt_202303081116036940450557780958073_0007_m_000000_8: Committed. Elapsed time: 3 ms.
23/03/08 11:16:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 8). 4953 bytes result sent to driver
23/03/08 11:16:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 1898 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:16:05 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/03/08 11:16:05 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.963 s
23/03/08 11:16:05 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/03/08 11:16:05 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.986088 s
23/03/08 11:16:05 INFO FileFormatWriter: Start to commit write Job e38f3589-ae86-44d3-b1a0-6dc19387122d.
23/03/08 11:16:05 INFO FileFormatWriter: Write Job e38f3589-ae86-44d3-b1a0-6dc19387122d committed. Elapsed time: 64 ms.
23/03/08 11:16:05 INFO FileFormatWriter: Finished processing stats for write job e38f3589-ae86-44d3-b1a0-6dc19387122d.
23/03/08 11:16:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/03/08 11:16:05 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
23/03/08 11:16:05 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:16:05 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:05 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:16:05 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:05 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 104.3 KiB, free 365.5 MiB)
23/03/08 11:16:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 365.5 MiB)
23/03/08 11:16:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.253.128:36715 (size: 37.6 KiB, free: 366.1 MiB)
23/03/08 11:16:05 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:16:05 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/03/08 11:16:05 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4664 bytes) taskResourceAssignments Map()
23/03/08 11:16:05 INFO Executor: Running task 0.0 in stage 8.0 (TID 9)
23/03/08 11:16:05 INFO Executor: Finished task 0.0 in stage 8.0 (TID 9). 1717 bytes result sent to driver
23/03/08 11:16:05 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 92 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:16:05 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/03/08 11:16:05 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.130 s
23/03/08 11:16:05 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/03/08 11:16:05 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.148958 s
23/03/08 11:16:05 INFO FileSourceStrategy: Pushed Filters: 
23/03/08 11:16:05 INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 11:16:05 INFO FileSourceStrategy: Output Data Schema: struct<age_group: string, subscription_count: bigint>
23/03/08 11:16:05 INFO CodeGenerator: Code generated in 34.62325 ms
23/03/08 11:16:05 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 357.5 KiB, free 365.1 MiB)
23/03/08 11:16:05 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 365.1 MiB)
23/03/08 11:16:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.253.128:36715 (size: 35.4 KiB, free: 366.1 MiB)
23/03/08 11:16:05 INFO SparkContext: Created broadcast 11 from showString at NativeMethodAccessorImpl.java:0
23/03/08 11:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:16:05 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
23/03/08 11:16:05 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:16:05 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:05 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:16:05 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:05 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.6 KiB, free 365.1 MiB)
23/03/08 11:16:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.1 MiB)
23/03/08 11:16:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.253.128:36715 (size: 6.1 KiB, free: 366.1 MiB)
23/03/08 11:16:05 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:16:05 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/03/08 11:16:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 10) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
23/03/08 11:16:05 INFO Executor: Running task 0.0 in stage 9.0 (TID 10)
23/03/08 11:16:06 INFO FileScanRDD: Reading File path: file:///home/subrat/age_subscription_count.parquet/part-00000-f1509966-a82e-4083-853b-8d6d52d43d78-c000.snappy.parquet, range: 0-847, partition values: [empty row]
23/03/08 11:16:06 INFO CodecPool: Got brand-new decompressor [.snappy]
23/03/08 11:16:06 INFO Executor: Finished task 0.0 in stage 9.0 (TID 10). 1788 bytes result sent to driver
23/03/08 11:16:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 10) in 213 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:16:06 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/03/08 11:16:06 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.255 s
23/03/08 11:16:06 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:16:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
23/03/08 11:16:06 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.266470 s
23/03/08 11:16:06 INFO CodeGenerator: Code generated in 50.227933 ms
+-----------+------------------+
|  age_group|subscription_count|
+-----------+------------------+
| Youngsters|              2939|
|    Seniors|               502|
|  Teenagers|                18|
|MiddleAgers|              1830|
+-----------+------------------+

23/03/08 11:16:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subscription_count),GreaterThan(subscription_count,2000)
23/03/08 11:16:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subscription_count#98L),(subscription_count#98L > 2000)
23/03/08 11:16:06 INFO FileSourceStrategy: Output Data Schema: struct<age_group: string, subscription_count: bigint>
23/03/08 11:16:06 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/03/08 11:16:06 INFO AvroUtils: Compressing Avro output using the snappy codec
23/03/08 11:16:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 11:16:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 11:16:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/03/08 11:16:06 INFO CodeGenerator: Code generated in 25.565938 ms
23/03/08 11:16:06 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 357.5 KiB, free 364.7 MiB)
23/03/08 11:16:06 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 364.7 MiB)
23/03/08 11:16:06 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.253.128:36715 (size: 35.4 KiB, free: 366.1 MiB)
23/03/08 11:16:06 INFO SparkContext: Created broadcast 13 from save at NativeMethodAccessorImpl.java:0
23/03/08 11:16:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:16:06 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
23/03/08 11:16:06 INFO DAGScheduler: Got job 8 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:16:06 INFO DAGScheduler: Final stage: ResultStage 10 (save at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:06 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:16:06 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:06 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[31] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:06 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 217.1 KiB, free 364.5 MiB)
23/03/08 11:16:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 364.4 MiB)
23/03/08 11:16:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.253.128:36715 (size: 77.2 KiB, free: 366.0 MiB)
23/03/08 11:16:06 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[31] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:16:06 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
23/03/08 11:16:06 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4989 bytes) taskResourceAssignments Map()
23/03/08 11:16:06 INFO Executor: Running task 0.0 in stage 10.0 (TID 11)
23/03/08 11:16:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 11:16:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 11:16:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/03/08 11:16:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.253.128:36715 in memory (size: 35.4 KiB, free: 366.0 MiB)
23/03/08 11:16:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.253.128:36715 in memory (size: 37.6 KiB, free: 366.1 MiB)
23/03/08 11:16:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.253.128:36715 in memory (size: 90.4 KiB, free: 366.2 MiB)
23/03/08 11:16:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.253.128:36715 in memory (size: 6.1 KiB, free: 366.2 MiB)
23/03/08 11:16:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.253.128:36715 in memory (size: 34.8 KiB, free: 366.2 MiB)
23/03/08 11:16:07 INFO FileScanRDD: Reading File path: file:///home/subrat/age_subscription_count.parquet/part-00000-f1509966-a82e-4083-853b-8d6d52d43d78-c000.snappy.parquet, range: 0-847, partition values: [empty row]
23/03/08 11:16:07 INFO FilterCompat: Filtering using predicate: and(noteq(subscription_count, null), gt(subscription_count, 2000))
23/03/08 11:16:07 INFO FileOutputCommitter: Saved output of task 'attempt_202303081116066019407708587011145_0010_m_000000_11' to file:/home/subrat/filtered_age_count.avro/_temporary/0/task_202303081116066019407708587011145_0010_m_000000
23/03/08 11:16:07 INFO SparkHadoopMapRedUtil: attempt_202303081116066019407708587011145_0010_m_000000_11: Committed. Elapsed time: 0 ms.
23/03/08 11:16:07 INFO Executor: Finished task 0.0 in stage 10.0 (TID 11). 2830 bytes result sent to driver
23/03/08 11:16:07 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 809 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:16:07 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
23/03/08 11:16:07 INFO DAGScheduler: ResultStage 10 (save at NativeMethodAccessorImpl.java:0) finished in 0.950 s
23/03/08 11:16:07 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:16:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
23/03/08 11:16:07 INFO DAGScheduler: Job 8 finished: save at NativeMethodAccessorImpl.java:0, took 0.964652 s
23/03/08 11:16:07 INFO FileFormatWriter: Start to commit write Job c0c3c3ef-71f8-4f8c-a94e-78cde03eb9bc.
23/03/08 11:16:07 INFO FileFormatWriter: Write Job c0c3c3ef-71f8-4f8c-a94e-78cde03eb9bc committed. Elapsed time: 49 ms.
23/03/08 11:16:07 INFO FileFormatWriter: Finished processing stats for write job c0c3c3ef-71f8-4f8c-a94e-78cde03eb9bc.
23/03/08 11:16:07 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.
23/03/08 11:16:07 INFO FileSourceStrategy: Pushed Filters: 
23/03/08 11:16:07 INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 11:16:07 INFO FileSourceStrategy: Output Data Schema: struct<age_group: string, subscription_count: bigint>
23/03/08 11:16:07 INFO CodeGenerator: Code generated in 32.423122 ms
23/03/08 11:16:07 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 353.3 KiB, free 365.3 MiB)
23/03/08 11:16:07 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.2 MiB)
23/03/08 11:16:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.253.128:36715 (size: 34.8 KiB, free: 366.2 MiB)
23/03/08 11:16:07 INFO SparkContext: Created broadcast 15 from showString at NativeMethodAccessorImpl.java:0
23/03/08 11:16:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 11:16:07 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
23/03/08 11:16:07 INFO DAGScheduler: Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/03/08 11:16:07 INFO DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)
23/03/08 11:16:07 INFO DAGScheduler: Parents of final stage: List()
23/03/08 11:16:07 INFO DAGScheduler: Missing parents: List()
23/03/08 11:16:07 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[35] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/03/08 11:16:08 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 12.0 KiB, free 365.2 MiB)
23/03/08 11:16:08 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
23/03/08 11:16:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.253.128:36715 (size: 6.0 KiB, free: 366.2 MiB)
23/03/08 11:16:08 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
23/03/08 11:16:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/03/08 11:16:08 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/03/08 11:16:08 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 12) (192.168.253.128, executor driver, partition 0, PROCESS_LOCAL, 4972 bytes) taskResourceAssignments Map()
23/03/08 11:16:08 INFO Executor: Running task 0.0 in stage 11.0 (TID 12)
23/03/08 11:16:08 INFO FileScanRDD: Reading File path: file:///home/subrat/filtered_age_count.avro/part-00000-72f6f072-7ce3-4ca1-b92e-60a5fc854859-c000.avro, range: 0-275, partition values: [empty row]
23/03/08 11:16:08 INFO CodeGenerator: Code generated in 26.233456 ms
23/03/08 11:16:08 INFO Executor: Finished task 0.0 in stage 11.0 (TID 12). 1491 bytes result sent to driver
23/03/08 11:16:08 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 12) in 115 ms on 192.168.253.128 (executor driver) (1/1)
23/03/08 11:16:08 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/03/08 11:16:08 INFO DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.145 s
23/03/08 11:16:08 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/03/08 11:16:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/03/08 11:16:08 INFO DAGScheduler: Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 0.160716 s
+----------+------------------+
| age_group|subscription_count|
+----------+------------------+
|Youngsters|              2939|
+----------+------------------+

23/03/08 11:16:08 INFO SparkContext: Invoking stop() from shutdown hook
23/03/08 11:16:08 INFO SparkUI: Stopped Spark web UI at http://192.168.253.128:4040
23/03/08 11:16:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/03/08 11:16:08 INFO MemoryStore: MemoryStore cleared
23/03/08 11:16:08 INFO BlockManager: BlockManager stopped
23/03/08 11:16:08 INFO BlockManagerMaster: BlockManagerMaster stopped
23/03/08 11:16:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/03/08 11:16:08 INFO SparkContext: Successfully stopped SparkContext
23/03/08 11:16:08 INFO ShutdownHookManager: Shutdown hook called
23/03/08 11:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-11bc94dd-ba50-4982-9361-a55e901f050e
23/03/08 11:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934/pyspark-365553ec-4314-4c34-810f-b75ef5ceb272
23/03/08 11:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-564bdab5-bae4-44c9-bd99-070e9ad28934
